#Implentation of https://arxiv.org/pdf/2106.09685
import json
import torch
import torch.nn.functional as F
from dataclasses import dataclass
from typing import Optional, Dict
import math
import logging
from torch.cuda.amp import autocast, GradScaler
from llama import xfmr, apply_rotary_emb, rms_norm, feed_forward
from weights import XfmrWeights, LayerWeights, load_weights
from kvcache import KVCache
from tokenizer import Tokenizer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)
@dataclass
class LoRAConfig:
    r: int = 8  # LoRA rank
    alpha: int = 16  # LoRA scaling factor
    dropout: float = 0.05

class CodeDataset(Dataset):
    def __init__(self, data_path: str, tokenizer, max_length: int = 2048):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.examples = []
        
        # Load and process data
        logging.info(f"Loading dataset from {data_path}")
        with open(data_path) as f:
            for line in f:
                example = json.loads(line)
                prompt = f"System: {example['system']}\nQuestion: {example['question']}\nResponse: "
                completion = example['response']
                
                # Tokenize
                full_text = prompt + completion
                tokens = self.tokenizer.encode(full_text)['input_ids']
                
                self.examples.append(tokens)
        
        logging.info(f"Loaded {len(self.examples)} examples")

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        return torch.tensor(self.examples[idx])

def create_attention_mask(seq_len: int, device: str = "cuda") -> torch.Tensor:
    """Create causal attention mask"""
    mask = torch.full((seq_len, seq_len), float("-inf"), device=device)
    mask = torch.triu(mask, diagonal=1)
    return mask

class GradientCheckpointFunction(torch.autograd.Function):
    """Custom checkpointing function for better memory efficiency"""
    @staticmethod
    def forward(ctx, run_function, length, *args):
        ctx.run_function = run_function
        ctx.save_for_backward(*args)
        ctx.length = length
        with torch.no_grad():
            return run_function(*args)

    @staticmethod
    def backward(ctx, *grad_outputs):
        if not any(grad_outputs):
            return (None, None) + tuple(None for _ in range(ctx.length))
        
        inputs = ctx.saved_tensors
        with torch.enable_grad():
            detached_inputs = [x.detach().requires_grad_() for x in inputs]
            outputs = ctx.run_function(*detached_inputs)
            
        if isinstance(outputs, torch.Tensor):
            outputs = (outputs,)
            
        torch.autograd.backward(
            outputs,
            grad_outputs,
            allow_unused=True
        )
        
        return (None, None) + tuple(inp.grad for inp in detached_inputs)

def checkpoint(function, *args):
    """Checkpoint wrapper"""
    return GradientCheckpointFunction.apply(function, len(args), *args)

class LoRALinearFunction:
    """LoRA adapter that preserves the original weights"""
    def __init__(self, weight: torch.Tensor, r: int = 8, alpha: int = 16, dropout: float = 0.05):
        self.weight = weight  # Original pretrained weight
        self.rank = r
        self.alpha = alpha
        self.scaling = alpha / r
        self.training = False
        
        # Initialize LoRA A and B matrices
        self.lora_A = torch.nn.Parameter(torch.zeros(weight.shape[1], r, device=weight.device))
        self.lora_B = torch.nn.Parameter(torch.zeros(r, weight.shape[0], device=weight.device))
        self.dropout = torch.nn.Dropout(p=dropout)
        
        # Initialize with scaled random weights
        torch.nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        torch.nn.init.zeros_(self.lora_B)

    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        # Regular forward pass with original weights
        base_output = F.linear(x, self.weight)
        
        # Add LoRA contribution during training
        if self.training:
            lora_output = (self.dropout(x) @ self.lora_A) @ self.lora_B * self.scaling
            return base_output + lora_output
        return base_output

    def parameters(self):
        yield self.lora_A
        yield self.lora_B

def get_lora_model(xfmr_weights: XfmrWeights, model_params, config: LoRAConfig):
    """Wraps the original xfmr function with LoRA layers"""
    
    # Create LoRA layers for attention weights
    lora_layers = {}
    for i in range(model_params.n_layers):
        layer = xfmr_weights.layer_weights[i]
        lora_layers[f'layer_{i}_wq'] = LoRALinearFunction(layer.wq, config.r, config.alpha)
        lora_layers[f'layer_{i}_wk'] = LoRALinearFunction(layer.wk, config.r, config.alpha)
        lora_layers[f'layer_{i}_wv'] = LoRALinearFunction(layer.wv, config.r, config.alpha)
        lora_layers[f'layer_{i}_wo'] = LoRALinearFunction(layer.wo, config.r, config.alpha)
    
    def lora_attention(x: torch.Tensor, layer_weights: LayerWeights, model_params, cur_pos: int, 
                      layer_idx: int, freqs_cis: torch.Tensor, kvcache: KVCache, 
                      attn_mask: Optional[torch.Tensor] = None):
        """Modified attention function using LoRA layers"""
        bsz, _, _ = x.shape
        n_rep = model_params.n_local_heads // model_params.n_local_kv_heads

        # Use LoRA layers instead of original linear layers
        xq = lora_layers[f'layer_{layer_idx}_wq'](x)
        xk = lora_layers[f'layer_{layer_idx}_wk'](x)
        xv = lora_layers[f'layer_{layer_idx}_wv'](x)
        
        # Reshape and continue with original attention logic
        xq = xq.reshape(bsz, -1, model_params.n_local_heads, model_params.head_dim)
        xk = xk.reshape(bsz, -1, model_params.n_local_kv_heads, model_params.head_dim)
        xv = xv.reshape(bsz, -1, model_params.n_local_kv_heads, model_params.head_dim)
        
        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
        keys, values, kvcache = kvcache.update(xk, xv, layer_idx, cur_pos, n_rep)
        
        xq = torch.permute(xq, (0, 2, 1, 3))
        keys = torch.permute(keys, (0, 2, 3, 1))
        values = torch.permute(values, (0, 2, 1, 3))
        
        scores = torch.matmul(xq.to(torch.bfloat16), keys)
        scores = scores / math.sqrt(model_params.head_dim)
        scores = scores.to(torch.float32)
        
        if cur_pos == 0:
            scores = scores + attn_mask
            
        scores = F.softmax(scores, dim=-1).to(torch.bfloat16)
        output = torch.matmul(scores, values)
        output = output.transpose(1, 2).reshape(bsz, -1, model_params.n_local_heads * model_params.head_dim)
        
        output = lora_layers[f'layer_{layer_idx}_wo'](output)
        return output, kvcache, scores
    
    def lora_forward(tokens: torch.Tensor, cur_pos: int, freqs_cis: torch.Tensor, 
                    kvcache: Optional[KVCache] = None, attn_mask: Optional[torch.Tensor] = None, 
                    training: bool = False):
        """Forward pass with LoRA layers"""
        # Set training mode for all LoRA layers
        for layer in lora_layers.values():
            layer.training = training
            
        h = xfmr_weights.tok_embeddings[tokens]
        
        for i in range(model_params.n_layers):
            norm_x = rms_norm(h, xfmr_weights.layer_weights[i].attention_norm)
            h_attn, kvcache, scores = lora_attention(
                norm_x, xfmr_weights.layer_weights[i], model_params,
                cur_pos, i, freqs_cis, kvcache, attn_mask
            )
            h = h + h_attn
            h = h + feed_forward(
                rms_norm(h, xfmr_weights.layer_weights[i].ffn_norm),
                xfmr_weights.layer_weights[i]
            )
            
        logits = F.linear(rms_norm(h, xfmr_weights.norm), xfmr_weights.output)
        return logits, kvcache, scores, None
    
    def get_lora_state_dict():
        """Get LoRA weights for saving"""
        return {name: {'lora_A': layer.lora_A, 'lora_B': layer.lora_B}
                for name, layer in lora_layers.items()}
    
    def load_lora_state_dict(state_dict):
        """Load saved LoRA weights"""
        for name, weights in state_dict.items():
            if name in lora_layers:
                lora_layers[name].lora_A.data = weights['lora_A']
                lora_layers[name].lora_B.data = weights['lora_B']
    
    def get_parameters():
        """Get trainable LoRA parameters"""
        for layer in lora_layers.values():
            yield from layer.parameters()
    
    return lora_forward, get_parameters, get_lora_state_dict, load_lora_state_dict

def setup_training(xfmr_weights, model_params, device="cuda"):
    """Setup LoRA training"""
    config = LoRAConfig(r=8, alpha=16, dropout=0.05)
    
    lora_forward, get_parameters, get_state_dict, load_state_dict = get_lora_model(
        xfmr_weights, model_params, config
    )
    
    optimizer = torch.optim.AdamW(
        get_parameters(),
        lr=1e-4,
        weight_decay=0.01
    )
    
    scaler = GradScaler()
    
    return {
        'forward': lora_forward,
        'optimizer': optimizer,
        'scaler': scaler,
        'save_weights': get_state_dict,
        'load_weights': load_state_dict
    }

def train_lora(
    xfmr_weights,
    model_params,
    train_data_path: str,
    output_dir: str,
    tokenizer_path: str,
    device: str = "cuda",
    batch_size: int = 1,
    gradient_accumulation_steps: int = 16,  # Increased for smaller batch size
    num_epochs: int = 3,
    learning_rate: float = 1e-4,
    max_grad_norm: float = 1.0,
    warmup_steps: int = 100,
    save_steps: int = 1000,
    max_length: int = 2048,
    checkpoint_factor: int = 4  # Split sequence into this many chunks for checkpointing
):
    """
    Memory-efficient LoRA training implementation
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # Initialize training components
    train_setup = setup_training(xfmr_weights, model_params, device)
    lora_forward = train_setup['forward']
    optimizer = train_setup['optimizer']
    scaler = train_setup['scaler']
    
    # Setup data
    tokenizer = tokenizer(tokenizer_path)
    train_dataset = CodeDataset(train_data_path, tokenizer, max_length)
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=1,  # Reduced for lower memory usage
        pin_memory=True
    )
    
    # Precompute rotary embeddings
    freqs_cis = precompute_freqs_cis(
        model_params.dim // model_params.n_heads,
        max_length * 2,
        device=device
    )
    
    def forward_chunk(chunk_tokens, chunk_pos, chunk_kvcache, chunk_mask):
        """Wrapper for checkpointed forward pass"""
        return lora_forward(
            chunk_tokens,
            cur_pos=chunk_pos,
            freqs_cis=freqs_cis[chunk_pos:chunk_pos + chunk_tokens.shape[1]],
            kvcache=chunk_kvcache,
            attn_mask=chunk_mask,
            training=True
        )[0]  # Only return logits
    
    # Training loop
    global_step = 0
    optimizer.zero_grad()
    best_loss = float('inf')
    
    for epoch in range(num_epochs):
        epoch_loss = 0
        num_batches = 0
        
        for batch_idx, tokens in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")):
            tokens = tokens.to(device)
            batch_size = tokens.shape[0]
            seq_len = tokens.shape[1]
            
            # Split sequence into chunks for checkpointing
            chunk_size = seq_len // checkpoint_factor
            if chunk_size == 0:
                chunk_size = seq_len
            
            accumulated_loss = 0
            
            # Process sequence in chunks
            for chunk_idx in range(0, seq_len, chunk_size):
                chunk_end = min(chunk_idx + chunk_size, seq_len)
                chunk_tokens = tokens[:, chunk_idx:chunk_end]
                
                # Initialize KV cache for this chunk
                chunk_kvcache = KVCache(
                    batch_size=batch_size,
                    max_seq_len=chunk_size,
                    n_layers=model_params.n_layers,
                    n_heads=model_params.n_heads,
                    head_dim=model_params.head_dim,
                    device=device
                )
                
                # Create attention mask for chunk
                chunk_mask = create_attention_mask(chunk_end - chunk_idx, device)
                
                # Forward pass with gradient checkpointing and mixed precision
                with autocast():
                    # Checkpoint the forward pass
                    logits = checkpoint(
                        forward_chunk,
                        chunk_tokens,
                        chunk_idx,
                        chunk_kvcache,
                        chunk_mask
                    )
                    
                    # Calculate loss for chunk
                    if chunk_end < seq_len:
                        target_tokens = tokens[:, chunk_idx+1:chunk_end+1]
                    else:
                        target_tokens = tokens[:, chunk_idx+1:chunk_end]
                        
                    chunk_loss = F.cross_entropy(
                        logits[:, :-1].reshape(-1, logits.size(-1)),
                        target_tokens.reshape(-1),
                    )
                    
                    # Scale loss
                    chunk_loss = chunk_loss / (gradient_accumulation_steps * checkpoint_factor)
                    accumulated_loss += chunk_loss.item()
                
                # Backward pass with gradient scaling
                scaler.scale(chunk_loss).backward()
                
                # Clear GPU memory
                del logits, chunk_loss, chunk_kvcache
                torch.cuda.empty_cache()
            
            # Gradient accumulation step
            if (batch_idx + 1) % gradient_accumulation_steps == 0:
                # Unscale and clip gradients
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(train_setup['get_parameters'](), max_grad_norm)
                
                # Optimizer step
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                
                # Learning rate warmup
                if global_step < warmup_steps:
                    lr = learning_rate * (global_step + 1) / warmup_steps
                    for param_group in optimizer.param_groups:
                        param_group['lr'] = lr
                
                # Logging
                epoch_loss += accumulated_loss
                num_batches += 1
                
                if global_step % 10 == 0:
                    avg_loss = accumulated_loss * gradient_accumulation_steps
                    logging.info(
                        f"Step {global_step}: loss = {avg_loss:.4f}, "
                        f"lr = {optimizer.param_groups[0]['lr']:.2e}"
                    )
                
                # Save checkpoint
                if global_step > 0 and global_step % save_steps == 0:
                    checkpoint_dir = output_dir / f"checkpoint-{global_step}"
                    checkpoint_dir.mkdir(exist_ok=True)
                    
                    avg_loss = epoch_loss / num_batches
                    if avg_loss < best_loss:
                        best_loss = avg_loss
                        is_best = True
                    else:
                        is_best = False
                    
                    # Save LoRA weights
                    checkpoint = {
                        'lora_weights': train_setup['save_weights'](),
                        'optimizer': optimizer.state_dict(),
                        'scaler': scaler.state_dict(),
                        'global_step': global_step,
                        'epoch': epoch,
                        'loss': avg_loss
                    }
                    
                    torch.save(checkpoint, checkpoint_dir / "checkpoint.pt")
                    if is_best:
                        torch.save(checkpoint, output_dir / "best_model.pt")
                
                global_step += 1
        
        # End of epoch logging
        avg_epoch_loss = epoch_loss / num_batches
        logging.info(f"Epoch {epoch+1} complete. Average loss: {avg_epoch_loss:.4f}")
    
    # Save final model
    torch.save(
        {
            'lora_weights': train_setup['save_weights'](),
            'optimizer': optimizer.state_dict(),
            'scaler': scaler.state_dict(),
            'global_step': global_step,
            'epoch': num_epochs,
            'loss': avg_epoch_loss
        },
        output_dir / "final_model.pt"
    )

def precompute_freqs_cis(dim: int, end: int, device: str = "cuda") -> torch.Tensor:
    """Precompute the frequency cis matrix for rotary embeddings."""
    freqs = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=device).float() / dim))
    t = torch.arange(end, device=device)
    freqs = torch.outer(t, freqs)
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    return freqs_cis

# Example usage
if __name__ == "__main__":
    train_lora(
        xfmr_weights=your_xfmr_weights,
        model_params=your_model_params,
        train_data_path="path/to/openorca-slim.jsonl",
        output_dir="lora_checkpoints",
        tokenizer_path="path/to/tokenizer",
        batch_size=1,
        gradient_accumulation_steps=16,
        checkpoint_factor=4  # Adjust based on your GPU memory
    )